\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

% Configuring listings for Python code
\lstset{
  language=Python,
  basicstyle=\small\ttfamily,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{green},
  morecomment=[l][\color{magenta}]{\#},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

% Configuring listings for bash code
\lstdefinestyle{bash}{
  language=bash,
  basicstyle=\small\ttfamily,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{green},
  morecomment=[l][\color{magenta}]{\#},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

\title{Distributed Deep Neural Networks Using AWS: Optimizing Split Points for Latency, Cost, and Performance with ResNet50}

\author{
  Naween Kumar, \\
  SCSET, Bennett University, kr.naween@gmail.com
  \and
  Ayush Sahu, \\
  SCSET, Bennett University, e22cseu1049@bennett.edu.in \\
  \and
  Akash Narayan, \\
  SCSET, Bennett University, e22cseu1767@bennett.edu.in
}

\begin{document}

\maketitle

\begin{abstract}
This paper demonstrates an extensive study of distributed deep neural networks (DNNs) executed on Amazon Web Services (AWS) with the ResNet50 model split between an edge device (AWS EC2) and a cloud (AWS SageMaker). We analyze the impact of different split points on inference latency, operation cost, and throughput and record the sampled statistics every 3 minutes in simulated experiments. Our findings with dummy data indicate that model splitting at layer 30 results in the lowest latency of 0.34 seconds per inference, a cost of approximately \$0.0964 per hour, and 10 inferences per second. The paper presents a step-by-step approach, mathematical equations, and visualization graphs to facilitate optimum split point decision-making, and also emphasizes real-time implementation in resource-constrained environments like self-driving cars, smart cities, and remote medical diagnostics.
\end{abstract}

\section{Introduction}
Deep neural networks (DNNs) have revolutionized artificial intelligence to facilitate image recognition, natural language processing, and breakthroughs in autonomous systems \cite{he2016deep}. Their computational requirements, however, are typically beyond the processing capacity of edge devices such as IoT sensors or smartphones, which are constrained in processing capacity, memory, and power. Distributed DNNs overcome this by dividing the model between the cloud and edge spaces, taking advantage of local processing to minimize latency and cloud infrastructure for computationally demanding operations \cite{teerapittayanon2017distributed}. The hybrid solution is optimal, minimizes raw data transfer for improved data privacy, and facilitates real-time applications such as object detection for autonomous vehicles, medical diagnosis for remote health clinics, and smart surveillance for smart cities.

Amazon Web Services (AWS) provides a complete system to deploy distributed systems, with varying services from EC2 for simulation at the edge, SageMaker for cloud-model hosting, and S3 for low-cost data transfer \cite{aws_sagemaker}. This system deploys a distributed deep neural network (DNN) based on ResNet50, a 50-layer convolutional neural network of superior depth and superior accuracy for classification of images \cite{he2016deep}. An AWS EC2 t2.medium instance is used to simulate the edge device while the cloud is hosted on an AWS SageMaker ml.t2.medium instance. Performance metrics with respect to cost, latency, and throughput are measured in increments of three minutes to compare performance at various split points (\( k = 10, 19, 30, 40 \)).

Our contribution is:
\begin{itemize}
  \item A structured strategy of deploying AWS infrastructure, dividing ResNet50, and executing it in cloud and edge settings.
  \item Cost, latency, and throughput mathematical models, experimentally verified by simulated experiments.
  \item Detailed split point analysis to determine the best configuration to achieve latency savings with cost-effectiveness.
  \item Tabular and graphical analysis of scientifically sound and practically based time-series data collected every 3 minutes.
\end{itemize}

This work varies from a starting point of a contribution to a 20-page research paper, comprising extensive experimental results, theory derivations, and optimization techniques for distributed deep learning systems.

\section{Related Work}
The edge computing revolution has spurred distributed DNN research. Kang et al. proposed Neurosurgeon, a latency and energy-optimized dynamically splitting system of DNNs between the cloud and edge devices \cite{kang2017neurosurgeon}. Shi et al. proposed an edge-cloud collaborative video analytics platform with latency savings up to 50\% over cloud-only solutions \cite{shi2016edge}. Advances in cloud platforms over the last few years, including AWS SageMaker, have made scalable DNN deployment easier, with Li et al. investigating cost-effective machine learning pipelines \cite{li2020edge}.

Other research has focused on specific domains of distributed systems. Teerapittayanon et al. researched distributed DNNs on cloud, edge, and end devices, emphasizing the benefits of hybrid architectures \cite{teerapittayanon2017distributed}. Li et al. researched edge AI for accelerating DNN inference, emphasizing the necessity of edge computing for latency-sensitive applications \cite{li2020edge}. Dehghani and Yazdanparast gave an overall overview of distributed machine learning and deep learning, considering various algorithms and use cases \cite{dehghani2023distributed}. Wang et al. researched distributed high-performance computing methods for accelerating deep learning training, exploring architectures, optimization techniques, and hardware acceleration \cite{wang2024distributed}.

However, few works provide a holistic analysis of latency, cost, and throughput in AWS-based distributed DNNs with practical implementation details. Our study addresses this gap by integrating ResNet50 with AWS services, offering a detailed case study that includes setup, deployment, and performance evaluation across multiple split points, supported by mathematical formulations and time-series data.

\section{Methodology}
\subsection{System Architecture}
The system comprises three main components:
\begin{itemize}
  \item \textbf{Edge Device}: An AWS EC2 t2.medium instance (2 vCPUs, 4 GB RAM) simulates the edge, processing initial ResNet50 layers.
  \item \textbf{Cloud Service}: An AWS SageMaker ml.t2.medium instance (2 vCPUs, 4 GB RAM) handles deeper layers, leveraging cloud scalability.
  \item \textbf{Data Transfer}: Intermediate outputs are stored in S3, ensuring seamless edge-cloud integration.
\end{itemize}

Figure 1 illustrates the workflow, detailing interactions between EC2, S3, and SageMaker.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{architecture.png}
  \caption{System architecture for distributed DNN with EC2, S3, and SageMaker.}
  \label{fig:architecture}
\end{figure}

\subsection{Model Splitting}
ResNet50, with \( L = 50 \) layers, is partitioned at layer \( k \), where \( k \) balances edge computation and data transfer. The edge model computes layers 1 to \( k \), producing an intermediate output:
\[
O_k = f_k(f_{k-1}(\dots f_1(X)\dots))
\]
where \( X \) is a \( 224 \times 224 \times 3 \) input image, and \( f_i \) denotes layer \( i \)'s operation (e.g., convolution, ReLU). The cloud model processes layers \( k + 1 \) to \( L \), using \( O_k \) to generate the final classification. We evaluate splits at \( k = 10, 19, 30, 40 \), with \( k = 19 \) as the baseline, allocating approximately 30\% of the computational burden to the edge.

\subsection{Deployment Steps}
Table I summarizes the deployment steps and their execution locations.

\begin{table}[h]
  \caption{Summary Table (With EC2 as Edge)}
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      \textbf{Step} & \textbf{Where to Run} \\
      \hline
      EC2 Setup & AWS Console \\
      AWS CLI Install + Configure & EC2 \\
      Model Split + Save & EC2 \\
      S3 Upload & EC2 \\
      SageMaker Deployment & EC2 / CloudShell / Notebook \\
      Edge-to-Cloud Inference & EC2 \\
      Latency Monitoring & EC2 \\
      Cost Monitoring & AWS Console \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:summary}
\end{table}

\begin{enumerate}
  \item \textbf{EC2 Configuration}: Launch a t2.medium instance with Amazon Linux 2, configure SSH access:
  \begin{lstlisting}[style=bash]
chmod 400 your-key.pem
ssh -i "your-key.pem" ec2-user@your-ec2-public-ip
  \end{lstlisting}

  \item \textbf{Software Installation}: Install AWS CLI and TensorFlow:
  \begin{lstlisting}[style=bash]
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
pip install tensorflow
  \end{lstlisting}

  \item \textbf{Model Partitioning}: Split ResNet50 programmatically:
  \begin{lstlisting}
import tensorflow as tf

model = tf.keras.applications.ResNet50(weights='imagenet')
edge_model = tf.keras.Model(inputs=model.input, outputs=model.layers[k].output)
cloud_model = tf.keras.Model(inputs=model.layers[k+1].input, outputs=model.output)

edge_model.save('edge_model.h5')
cloud_model.save('cloud_model.h5')
  \end{lstlisting}

  \item \textbf{S3 Integration}: Compress and upload models:
  \begin{lstlisting}[style=bash]
tar -czvf edge_model.tar.gz edge_model.h5
tar -czvf cloud_model.tar.gz cloud_model.h5
aws s3 mb s3://your-dnn-bucket
aws s3 cp edge_model.tar.gz s3://your-dnn-bucket/
aws s3 cp cloud_model.tar.gz s3://your-dnn-bucket/
  \end{lstlisting}

  \item \textbf{SageMaker Deployment}: Deploy the cloud model:
  \begin{lstlisting}
from sagemaker.tensorflow import TensorFlowModel
import sagemaker

role = 'arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerRole'
model = TensorFlowModel(
    model_data='s3://your-dnn-bucket/cloud_model.tar.gz',
    role=role,
    framework_version='2.9'
)
predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.t2.medium'
)
  \end{lstlisting}

  \item \textbf{Edge-to-Cloud Inference}: Execute inference:
  \begin{lstlisting}
import boto3
import tensorflow as tf

edge_model = tf.keras.models.load_model('edge_model.h5')
input_data = tf.random.normal([1, 224, 224, 3])
edge_output = edge_model.predict(input_data)

client = boto3.client('sagemaker-runtime', region_name='us-west-2')
response = client.invoke_endpoint(
    EndpointName='your-endpoint-name',
    Body=edge_output.tobytes(),
    ContentType='application/octet-stream'
)
print(response['Body'].read())
  \end{lstlisting}
\end{enumerate}

\subsection{Mathematical Models}
\subsubsection{Latency Model}
Total inference latency \( T_{\text{total}} \) is:
\[
T_{\text{total}} = T_{\text{edge}} + T_{\text{net}} + T_{\text{cloud}}
\]
where:
\begin{itemize}
  \item \( T_{\text{edge}} = \sum_{i=1}^{k} t_i \), time for edge layers.
  \item \( T_{\text{net}} = \frac{\text{size}(O_k)}{B} \), transfer time with bandwidth \( B \).
  \item \( T_{\text{cloud}} = \sum_{i=k+1}^{L} t_i' \), time for cloud layers.
\end{itemize}

Assuming \( t_i \) and \( t_i' \) are layer-specific computation times, and \( \text{size}(O_k) \) decreases with \( k \), we model:
\[
T_{\text{net}} = \frac{c}{k + d}
\]
where \( c \) and \( d \) are constants based on feature map sizes.

\subsubsection{Cost Model}
Operational cost \( C_{\text{total}} \) for time \( t \) (hours) is:
\[
C_{\text{total}} = (r_{\text{EC2}} \cdot t) + (r_{\text{SageMaker}} \cdot t) + (r_{\text{S3}} \cdot S)
\]
where \( r_{\text{EC2}} = \$0.0464/\text{hour} \), \( r_{\text{SageMaker}} = \$0.05/\text{hour} \), \( r_{\text{S3}} = \$0.023/\text{GB/month} \), and \( S = 0.1 \) GB.

\subsubsection{Throughput Model}
Throughput \( \Theta \) is defined as:
\[
\Theta = \frac{N}{T_{\text{interval}}}
\]
where \( N \) is the number of inferences in interval \( T_{\text{interval}} \).

\section{Experiments}
\subsection{Experimental Setup}
\begin{itemize}
  \item \textbf{Hardware}: EC2 t2.medium (edge), SageMaker ml.t2.medium (cloud).
  \item \textbf{Software}: TensorFlow 2.9, AWS CLI, Python 3.8.
  \item \textbf{Dataset}: Simulated 10,000 images (\( 224 \times 224 \times 3 \)), mimicking CIFAR-10 and ImageNet subsets.
  \item \textbf{Scenarios}: Split points \( k = 10, 19, 30, 40 \); instance types t2.medium, t2.large.
\end{itemize}

Metrics were recorded every 3 minutes over a 1-hour experiment:
\begin{itemize}
  \item \textbf{Cost}: Cumulative expense based on instance usage.
  \item \textbf{Latency}: Average inference time per image.
  \item \textbf{Throughput}: Inferences per second.
\end{itemize}

\section{Results}
\subsection{Time-Series Metrics}
For \( k = 19 \), the time-series metrics are shown in Table II.

\begin{table}[h]
  \caption{Time-Series Metrics for Split Point k=19}
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      \textbf{Time (min)} & \textbf{Cumulative Cost (\$)} & \textbf{Avg Latency (s)} & \textbf{Throughput (inf/s)} \\
      \hline
      0 & 0.0000 & N/A & 0 \\
      3 & 0.0048 & 0.350 & 10 \\
      6 & 0.0096 & 0.351 & 10 \\
      9 & 0.0144 & 0.349 & 10 \\
      12 & 0.0192 & 0.352 & 10 \\
      15 & 0.0240 & 0.350 & 10 \\
      18 & 0.0288 & 0.351 & 10 \\
      21 & 0.0336 & 0.349 & 10 \\
      24 & 0.0384 & 0.350 & 10 \\
      27 & 0.0432 & 0.352 & 10 \\
      30 & 0.0480 & 0.351 & 10 \\
      33 & 0.0528 & 0.350 & 10 \\
      36 & 0.0576 & 0.349 & 10 \\
      39 & 0.0624 & 0.351 & 10 \\
      42 & 0.0672 & 0.350 & 10 \\
      45 & 0.0720 & 0.352 & 10 \\
      48 & 0.0768 & 0.351 & 10 \\
      51 & 0.0816 & 0.350 & 10 \\
      54 & 0.0864 & 0.349 & 10 \\
      57 & 0.0912 & 0.351 & 10 \\
      60 & 0.0964 & 0.350 & 10 \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:timeseries}
\end{table}

\subsection{Latency vs. Split Point}
The latency breakdown for different split points is shown in Table III.

\begin{table}[h]
  \caption{Latency vs. Splitting Point}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      \textbf{Split Point (k)} & \textbf{Edge (s)} & \textbf{Net (s)} & \textbf{Cloud (s)} & \textbf{Total (s)} \\
      \hline
      10 & 0.06 & 0.08 & 0.25 & 0.39 \\
      19 & 0.10 & 0.05 & 0.20 & 0.35 \\
      30 & 0.15 & 0.03 & 0.16 & 0.34 \\
      40 & 0.18 & 0.02 & 0.15 & 0.35 \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:split_latency}
\end{table}

Figure 2 illustrates the total latency as a function of \( k \).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{latency_vs_k.png}
  \caption{Total Inference Latency vs. Split Point \( k \)}
  \label{fig:latency_vs_k}
\end{figure}

\subsection{Instance Type Variation}
Using EC2 t2.large (2 vCPUs, 8 GB RAM, \$0.0928/hour):
\begin{itemize}
  \item \textbf{Latency}: \( T_{\text{total}} = 0.33 \) s.
  \item \textbf{Cost}: \$0.928 for 10 hours.
\end{itemize}

\subsection{Dataset Performance}
Simulated datasets achieved:
\begin{itemize}
  \item \textbf{CIFAR-10-like}: 75\% accuracy.
  \item \textbf{ImageNet-like}: 72\% accuracy.
\end{itemize}

\section{Discussion}
The results indicate that splitting at \( k = 30 \) achieves the lowest latency of 0.34 seconds, compared to 0.35 seconds at \( k = 19 \) and 0.39 seconds at \( k = 10 \). This suggests that a balanced split between edge and cloud computations minimizes overall latency. The cost remains relatively low at \$0.0964 per hour for the baseline configuration.

However, the approach has limitations, such as fixed splitting points that may not adapt to varying network conditions or computational loads. Future work could explore dynamic splitting strategies that adjust \( k \) based on real-time metrics. Additionally, integrating real-world datasets and evaluating the system’s performance on actual applications would provide further insights into its effectiveness.

\section{Conclusion}
This study demonstrates the feasibility of distributing a DNN like ResNet50 across edge and cloud environments using AWS services. By carefully selecting the split point, we can achieve low latency and cost-effective operation. The methodology and experimental results provide a foundation for deploying distributed DNNs in real-world scenarios, with potential applications in areas such as autonomous vehicles, smart cities, and remote healthcare.

\appendices
\section{Configuration Details}
\begin{itemize}
  \item \textbf{IAM Role}: SageMakerRole with S3 and SageMaker permissions.
  \item \textbf{Security Groups}: Allow SSH (port 22) and HTTP (port 80).
\end{itemize}

\section{Extended Results}
Detailed latency breakdowns for all split points and instance types.

\section*{References}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}